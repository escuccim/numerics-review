{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review-06-Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers to review questions from Chapter 6: Optimization <cite data-cite=\"heath2018scientific\">(Heath, 2018)</cite>.\n",
    "\n",
    "---\n",
    "Questions marked with $\\bigtriangledown$ are considered more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.1. True or false: Points that minimize a non- linear function are inherently less accurately de- termined than points for which a nonlinear func- tion has a zero value.\n",
    "\n",
    "True. In comparison to solving nonlinear equations, minima can only be computed to half precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.2. True or false: If a function is unimodal on a closed interval, then it has exactly one minimum on the interval.\n",
    "\n",
    "True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.3. True or false: In minimizing a unimodal function of one variable by golden section search, the point discarded at each iteration is always the point having the largest function value.\n",
    "\n",
    "True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.4. True or false: For minimizing a real-valued function of several variables, the steepest descent method is usually more rapidly convergent than Newton’s method.\n",
    "\n",
    "False. Newton's method has quadratic convergence ($r = 2$) whereas steepest descent has linear convergence ($r = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.7. Suppose that the real-valued function f is unimodal on the interval $[a,b]$. Let x1 and x2 be two points in the interval, with $a < x1 < x2 < b$. If f(x1) = 1.232 and f(x2) = 3.576, then which of the following statements is valid?\n",
    "\n",
    "1. The minimum of f must lie in the subinterval $[x1 , b]$.\n",
    "\n",
    "2. The minimum of f must lie in the subinterval $[a, x2]$.\n",
    "\n",
    "3. One can’t tell which of these two subintervals the minimum must lie in without knowing the values of f(a) and f(b).\n",
    "\n",
    "\n",
    "Answer is 2. The minimum must lie in the subinterval $[a, x2]$.  Since $f(x1) < f(x2)$ we know that the subinterval containing $[x2, b]$ can be discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.8. (a) In minimizing a unimodal function of one variable on the interval $[0,1]$ by golden sec- tion search, at what two points in the interval is the function initially evaluated?\n",
    "(b) Why are those particular points chosen?\n",
    "\n",
    "(a) Initially the points $\\tau$ and $1 - \\tau$ are evaluated where $\\tau = (\\sqrt(5) - 1) / 2$. In other words, $[0.382, 0.618]$.\n",
    "\n",
    "(b) No matter which subinterval is retained, the length of that subinterval will be $\\tau$ of the previous interval.  This means that we only need to evaluate the function at 1 new point per iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.9. If the real-valued function f is monotonic on the interval $[a, b]$, will golden section search to find a minimum of f still converge? If not, why, and if so, to what point?\n",
    "\n",
    "A monotonic function is not necessarily strictly increasing or decreasing.  For example, a monotonic function can be horizontal for some interval and if the interval $[a, b]$ falls completely within a segement of the function which is horizontal, then golden section search will not converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.10. Suppose that the real-valued function f is unimodal on the interval $[a,b]$, and x1 and x2 are points in the interval such that x1 < x2 and f(x1) < f(x2). (a) What is the shortest interval in which you know that the minimum of f must lie? (b) How would your answer change if we happened to have f(x1) = f(x2)?\n",
    "\n",
    "(a) The shortest interval in which x must lie is $[a, x2]$ since we discard $[x2, b]$.\n",
    "\n",
    "(b) If $f(x1) = f(x2)$, then we haven't gained any information about the minimum and thus shortest interval is the original interval $[a, b]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.11. List one advantage and one disadvantage of golden section search compared with successive parabolic interpolation for minimizing a function of one variable.\n",
    "\n",
    "Advantage\n",
    "* Faster convergence ($r \\approx 1.324$). \n",
    "\n",
    "Disadvantage\n",
    "* Not guaranteed to converge if not started close enough to minimum whereas golden section search is guaranteed to converge.\n",
    "\n",
    "Incidentally, these tradeoffs are identical to bisection method and secant method for finding roots of nonlinear equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.13. For minimizing a function f : R → R, successive parabolic interpolation and Newton’s method both fit a quadratic polynomial to the function f and then take its minimum as the next approximate solution. (a) How do these two methods differ in choosing the quadratic polynomials they use? (b) What difference does this make in their respec- tive convergence rates?\n",
    "\n",
    "(a) Newton's method uses a local quadratic approximation to the function being minimized based on a truncated Taylor series.  In contrast, successive parabolic interpolation uses a parabola which might not accurately reflect the behavior of the function at the minimum.\n",
    "\n",
    "(b) Newton's method converges more quickly ($r = 2$) in comparison to successive parabolic interpolation ($r \\approx 1.324$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.15. Suppose you want to minimize a function of one variable, f : R → R. For each convergence rate given, name a method that normally has that convergence rate for this problem:\n",
    "\n",
    "(a) Linear but not superlinear\n",
    "\n",
    "Golden section search\n",
    "\n",
    "(b) Superlinear but not quadratic\n",
    "\n",
    "Successive parabolic interpolation\n",
    "\n",
    "(c) Quadratic\n",
    "\n",
    "Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.17. Which of the following iterative methods have a superlinear convergence rate under normal circumstances?\n",
    "\n",
    "(a) Successive parabolic interpolation for minimiz- ing a function\n",
    "\n",
    "Successive parabolic interpolation has superlinear convergence ($r \\approx 1.324$).\n",
    "\n",
    "(b) Golden section search for minimizing a func- tion\n",
    "\n",
    "Golden section search has linear convergence ($r = 1$)\n",
    "\n",
    "(c) Interval bisection for finding a zero of a func- tion\n",
    "\n",
    "Interval bisection has linear convergence ($r = 1$)\n",
    "\n",
    "(d) Secant updating methods for minimizing a function of n variables\n",
    "\n",
    "Secant updating methods such as BFGS have superlinear convergence ($1 < r < 2$)\n",
    "\n",
    "(e) Steepest descent method for minimizing a function of n variables\n",
    "\n",
    "Steepest descent is an unconstrained optimization algorithm which uses negative gradient $-\\nabla f(x)$ and has linear convergence ($r = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.19. For minimizing a quadratic function of n variables, what is the maximum number of it- erations required to converge to the exact solu- tion (assuming exact arithmetic) from an arbi- trary starting point for each of the following al- gorithms?\n",
    "\n",
    "(a) Conjugate gradient method\n",
    "\n",
    "Not sure.\n",
    "\n",
    "(b) Newton’s method\n",
    "\n",
    "Newton's method will require exactly 1 iteration to converge to the minimum of a quadratic function.\n",
    "\n",
    "(c) BFGS secant updating method with exact line search\n",
    "\n",
    "Not sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.21. Let $f : \\mathbb{R}^2 → \\mathbb{R}$ be a real-valued function of two variables. What is the geometrical interpre- tation of the gradient vector? \n",
    "$$\n",
    "\\nabla f(x) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f(x)}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f(x)}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The sign of the gradient gives the direction and magnitude of the rate of change with respect to a given variable.  For example, a negative gradient can be interpreted as the \"downhill\" direction and a large magnitude can be interpreted as relatively \"steep\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 6.23. The steepest descent method for minimiz- ing a function of several variables is usually slow but reliable. However, it can sometimes fail, and it can also sometimes converge rapidly. Under what conditions would each of these two types of behav- ior occur?\n",
    "\n",
    "Steepest descent uses negative gradient $-\\nabla f(x)$ along with a line search to determine how far to travel during the current iteration.  The resulting iterates produce a characteristic zig-zag pattern when minimizing a function in 2d.  This zig-zag demonstrates the inefficiency of the method since the method is constrained to make changes in direction which are perpindicular to the previous direction.  The steeper the gradient the more pronounced the zig-zag and less efficient to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
